{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables:\n",
    "\n",
    "- flag superficie imputada\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from malbecs.modeling import train as tr\n",
    "\n",
    "wine_path = \"../../data/final/wine_final.csv\"\n",
    "eto_path = \"../../data/final/eto_final.csv\"\n",
    "meteo_path = \"../../data/final/meteo_final.csv\"\n",
    "\n",
    "def show_feat_imps(feat_imp, feat_names):\n",
    "    pd.DataFrame(\n",
    "        feat_imp,\n",
    "        index=feat_names,\n",
    "        columns=[\"feat_imp\"]\n",
    "    ).sort_values(\"feat_imp\")[-50:].plot(kind='barh', figsize=(6, 15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = tr.load_final_data(\n",
    "    wine_path=wine_path,\n",
    "    eto_path=eto_path,\n",
    "    meteo_path=meteo_path\n",
    ")\n",
    "\n",
    "with open(\"../../data/final/meteo_features.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    meteo_cols = f.read().split(\"\\n\")\n",
    "\n",
    "with open(\"../../data/final/eto_features.txt\", \"r\") as f:\n",
    "    eto_cols = f.read().split(\"\\n\")\n",
    "\n",
    "with open(\"../../data/final/wine_features.txt\", \"r\") as f:\n",
    "    wine_cols = f.read().split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = tr.filter_camp(data.copy(), min_camp=15, max_camp=21)\n",
    "\n",
    "data_final = tr.filter_camp(data, min_camp=22, max_camp=22)\n",
    "\n",
    "train, test = tr.train_test_split(data_train, test_camp=21)\n",
    "\n",
    "X, y = tr.xy_split(data_train)\n",
    "\n",
    "cat_cols = [\n",
    "    'id_finca',\n",
    "    'id_zona',\n",
    "    'id_estacion',\n",
    "    'variedad',\n",
    "    \"modo\",\n",
    "    \"tipo\",\n",
    "    \"color\",\n",
    "    \"prod_shift1_gt_shift2\",\n",
    "    \"sup_is_nan\",\n",
    "]\n",
    "num_cols = [col for col in X.columns if col not in cat_cols]\n",
    "\n",
    "X[cat_cols] = X[cat_cols].astype('category')\n",
    "\n",
    "X_train, y_train = tr.xy_split(train)\n",
    "X_test, y_test = tr.xy_split(test)\n",
    "X_final, y_final = tr.xy_split(data_final)\n",
    "\n",
    "train_idxs, test_idxs = tr.CampKFold.get_train_test(\n",
    "    X['campaña'], from_camp=19, to_camp=21\n",
    ")\n",
    "\n",
    "cv = tr.CampKFold(train_idxs, test_idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer, OrdinalEncoder, StandardScaler\n",
    "import pickle as pkl\n",
    "from typing import List\n",
    "import malbecs.modeling.transformers as mt\n",
    "\n",
    "seed = 42\n",
    "\n",
    "def get_base_model():\n",
    "\n",
    "    model_num_cols = [\n",
    "        'superficie',\n",
    "        'prod_shift_max',\n",
    "        'prod_shift_change',\n",
    "        'prod_shift_avg',\n",
    "    ]\n",
    "\n",
    "    m = make_pipeline(\n",
    "        make_column_transformer(\n",
    "\n",
    "            (mt.BaseNEncoder(), ['id_finca']),\n",
    "\n",
    "            (mt.TargetEncoder(), ['id_zona']),\n",
    "\n",
    "            (OrdinalEncoder(handle_unknown='use_encoded_value',\n",
    "             unknown_value=-1), ['id_estacion']),\n",
    "\n",
    "            (mt.BaseNEncoder(), ['variedad']),\n",
    "\n",
    "            (OrdinalEncoder(handle_unknown='use_encoded_value',\n",
    "             unknown_value=-1), ['modo']),\n",
    "\n",
    "            (KBinsDiscretizer(n_bins=2, random_state=seed), ['altitud']),\n",
    "\n",
    "            (StandardScaler(), model_num_cols),\n",
    "\n",
    "            remainder='drop'\n",
    "        ),\n",
    "        RandomForestRegressor(\n",
    "            random_state=seed,\n",
    "            n_estimators=200,\n",
    "            min_samples_leaf=4,\n",
    "            n_jobs=-1,\n",
    "            max_features='sqrt',\n",
    "            max_samples=0.8\n",
    "        )\n",
    "    )\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  [-4950.42915184 -4769.7101927  -4818.80517865]\n",
      "test:  [-4867.31956362 -6767.87298344 -5430.5269726 ]\n",
      "Train Mean RMSE: -4846.314841064502\n",
      "Test Mean RMSE: -5688.573173223823\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "\n",
    "res = cross_validate(\n",
    "    estimator=m,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    scoring=tr.rmse_scorer,\n",
    "    return_train_score=True,\n",
    "    return_estimator=True\n",
    ")\n",
    "\n",
    "\n",
    "print(\"train: \", res['train_score'])\n",
    "print(\"test: \", res['test_score'])\n",
    "print(f\"Train Mean RMSE: {np.mean(res.get('train_score'))}\")\n",
    "print(f\"Test Mean RMSE: {np.mean(res.get('test_score'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "percip_cols = [c for c in eto_cols if \"Precip\" in c]\n",
    "snow_cols = [c for c in eto_cols if \"Snow\" in c]\n",
    "temp_day_cols = [c for c in eto_cols if \"TemperatureLocalDay\" in c]\n",
    "temp_cols = [\n",
    "    c for c in eto_cols \n",
    "    if \"TemperatureLocalAfter\" in c or \"TemperatureLocalOvern\" in c\n",
    "]\n",
    "evotrans_cols = [c for c in eto_cols if \"Evapotranspiration\" in c]\n",
    "feelslike_cols = [c for c in eto_cols if \"FeelsLikeLoca\" in c]\n",
    "irrad_cols = [c for c in eto_cols if \"Irradiance\" in c]\n",
    "gust_cols = [c for c in eto_cols if \"Gust\" in c]\n",
    "wind_cols = [c for c in eto_cols if \"Wind\" in c]\n",
    "dewpoint_cols = [c for c in eto_cols if \"Dewpoint\" in c]\n",
    "mslp_cols = [c for c in eto_cols if \"MSLP\" in c]\n",
    "humid_cols = [c for c in eto_cols if \"Humidity\" in c]\n",
    "uvindex_cols = [c for c in eto_cols if \"UVIndex\" in c]\n",
    "visib_cols = [c for c in eto_cols if \"Visibility\" in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempuo = [c for c in eto_cols if (\"Temp\" in c) and (\"StdM\" in c )]\n",
    "precipuo = [c for c in eto_cols if (\"Precip\" in c) and (\"StdM\" in c )]\n",
    "snowpuo = [c for c in eto_cols if (\"Snow\" in c) and (\"StdM\" in c )]\n",
    "winduo = [c for c in eto_cols if (\"Wind\" in c) and (\"StdM\" in c )]\n",
    "gustuo = [c for c in eto_cols if (\"Gust\" in c) and (\"StdM\" in c)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_fincas = X['id_finca'].nunique()\n",
    "# top_fincas = X.id_finca.value_counts()[:int(0.8*n_fincas)].index.values.tolist()\n",
    "# X['id_finca_top'] = X['id_finca'].apply(lambda x: x if x in top_fincas else -1)\n",
    "\n",
    "# n_zonas = X['id_zona'].nunique()\n",
    "# top_zonas = X['id_zona'].value_counts()[:int(0.8*n_zonas)].index.values.tolist()\n",
    "# X['id_zona_top'] = X['id_zona'].apply(lambda x: x if x in top_zonas else -1)\n",
    "\n",
    "# n_var = X['variedad'].nunique()\n",
    "# top_variedad= X['variedad'].value_counts()[:int(0.8*n_var)].index.values.tolist()\n",
    "# X['variedad_top'] = X['variedad'].apply(\n",
    "#     lambda x: x if x in top_variedad else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mt.QuantileFeatureEncoder(qs=np.linspace(0.1, 0.9, 9).tolist(), col='id_finca').fit_transform(X,y)['id_finca'].value_counts()\n",
    "# X.filter(like='sup')\n",
    "# X[[c for c in meteo_cols if \"avg\" in c ]].hist(figsize=(15,15))\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# cols_pca = [c for c in gust_cols if 'Max' in c]\n",
    "# pca = make_pipeline(StandardScaler(),PCA(random_state=seed))\n",
    "\n",
    "\n",
    "# wine_num_cols\n",
    "\n",
    "# X[wind_cols].hist(figsize=(15,15))\n",
    "# X[[c for c in humid_cols if \"Avg\" in c]].hist(figsize=(15,15))\n",
    "# from sklearn.decomposition import PCA, KernelPCA\n",
    "# pca = make_pipeline(StandardScaler(),PCA())\n",
    "\n",
    "# Xt = pca.fit_transform(X[cols_pca])\n",
    "\n",
    "\n",
    "# plot = plt.scatter(Xt[:,0], Xt[:,1])\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(\n",
    "#     range(1,pca[-1].explained_variance_ratio_.shape[0]+1),\n",
    "#     np.cumsum(pca[-1].explained_variance_ratio_),\n",
    "# )\n",
    "# temp_cols\n",
    "# [c for c in snowpuo if \"Month1\" in c or \"Month2\" in c]\n",
    "\n",
    "\n",
    "# X[precipuo].hist(figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.base import BaseEstimator, OneToOneFeatureMixin, TransformerMixin\n",
    "\n",
    "\n",
    "# class GroupedQuantileFeatureEncoder(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):\n",
    "\n",
    "#     def __init__(self, col: str, groups:List[str], value:str, qs=[0.25, 0.5, 0.75], scale=True):\n",
    "#         self.col = col\n",
    "#         self.groups = groups\n",
    "#         self.value = value\n",
    "#         self.qs = qs\n",
    "#         self.scale = scale\n",
    "        \n",
    "\n",
    "#     def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "\n",
    "#         X = X.copy()\n",
    "\n",
    "#         if not isinstance(X, pd.DataFrame):\n",
    "#             return Exception(\"X must be of type pd.DataFrame\")\n",
    "#         if not isinstance(y, pd.Series):\n",
    "#             return Exception(\"y must be of type pd.Series\")\n",
    "\n",
    "    \n",
    "#         self.category_means_ = X.groupby(self.groups)[self.value].mean()\n",
    "#         # category_means_ = pd.concat([X[self.col], y], axis=1).groupby(self.col)[\n",
    "#             # y.name].mean()\n",
    "\n",
    "#         self.qs_ = [self.category_means_.quantile(q) for q in self.qs]\n",
    "\n",
    "#         def encode_qs(x, qs):\n",
    "#             for i, q in enumerate(qs):\n",
    "#                 if x < q:\n",
    "#                     return i\n",
    "#             return len(qs)\n",
    "\n",
    "#         self.category_encodings_ = self.category_means_.apply(\n",
    "#             lambda x: encode_qs(x, self.qs_)\n",
    "#         ).to_dict()\n",
    "\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         X = X.copy()\n",
    "#         X.groupby(self.groups)[self.col].map(self.category_encodings_)\n",
    "#         # X[self.col] = X[self.col].map(self.category_encodings_\n",
    "#         X[self.col] = X[self.col].fillna(-1)\n",
    "#         return X\n",
    "    \n",
    "\n",
    "# gqe = GroupedQuantileFeatureEncoder(\n",
    "#     col=\"id_finca\",\n",
    "#     groups=[\"variedad\",\"modo\"],\n",
    "#     value=\"prod_he_shift1\"\n",
    "# )\n",
    "\n",
    "# gqe.fit(X,y)\n",
    "\n",
    "# # gqe.category_encodings_\n",
    "# # gqe.mean_\n",
    "# # X['prod_he_shift1']\n",
    "# xcopy = X.copy()\n",
    "# # xcopy['a'] = \n",
    "# xcopy.set_index(gqe.groups).index.map(gqe.category_encodings_)\n",
    "# # xcopy['a']\n",
    "# # .transform(map(gqe.category_encodings_))\n",
    "# # gqe.category_encodings_\n",
    "# # gqe.category_encodings_\n",
    "# # gqe.category_means_\n",
    "# gqe.qs_\n",
    "\n",
    "# # X.groupby([\"variedad\",\"campaña\"])[\"prod_he_shift1\"].mean()\n",
    "# # X.groupby([\"id_finca\",\"campaña\"])[\"prod_he_shift1\"].mean()\n",
    "# # X.groupby([\"id_estacion\",\"campaña\"])[\"prod_he_shift1\"].mean()\n",
    "# # X['prod_he_shift1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.groupby([\"id_finca\",\"campaña\"])['prod_he_shift1'].quantile(0.25)\n",
    "\n",
    "# X.groupby([\"variedad\",\"modo\"])['prod_he_shift1'].quantile(0.9)\n",
    "# X.groupby([\"variedad\",\"modo\"])['prod_he_shift1'].quantile(0.25)\n",
    "\n",
    "# X.set_index(['id_finca','variedad',\"modo\"])['prod_he_shift1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def get_preprocesing():\n",
    "\n",
    "    model_num_cols = [\n",
    "        'altitud',\n",
    "        'campaña',\n",
    "        'superficie',\n",
    "\n",
    "        \"prod_shift1\",\n",
    "        'prod_shift_max',\n",
    "        'prod_shift_change',\n",
    "        'prod_shift_avg',\n",
    "\n",
    "        'prod_he_var_zone_mean_hist_total',\n",
    "        'prod_he_var_zone_std_hist_total',\n",
    "\n",
    "        'prod_he_var_modo_zona_mean_shift1_total',\n",
    "        \"prod_he_var_modo_zona_change_total\",\n",
    "\n",
    "        \"prod_he_var_mean_shift1_total\",\n",
    "        \"prod_he_var_change_total\",\n",
    "\n",
    "        \"prod_he_var_modo_mean_shift1_total\",\n",
    "        \"prod_he_var_modo_change_total\",\n",
    "\n",
    "    ]\n",
    "\n",
    "    return make_column_transformer(\n",
    "\n",
    "        (OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1), ['sup_is_nan']),\n",
    "\n",
    "        # (mt.BaseNEncoder(), ['id_finca']),\n",
    "        # (mt.TargetEncoder(), ['id_finca']),\n",
    "        # (mt.QuantileFeatureEncoder(col='id_finca'), ['id_finca']),\n",
    "        # (OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ['id_finca']),\n",
    "        # (OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1), ['id_finca_top']),\n",
    "         \n",
    "        # (mt.TargetEncoder(), ['id_zona']),\n",
    "        (OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1), ['id_zona']),\n",
    "\n",
    "        # (OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1), ['id_estacion']),\n",
    "\n",
    "        # (mt.BaseNEncoder(), ['variedad']), \n",
    "        (OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1), ['variedad']),\n",
    "        # (mt.QuantileFeatureEncoder(qs=np.linspace(0.1, 0.9, 9).tolist(), col='variedad'), ['variedad']),\n",
    "\n",
    "        (OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1), ['modo']),\n",
    "\n",
    "        # (KBinsDiscretizer(n_bins=3, encode='ordinal'), ['altitud']),\n",
    "        # (KBinsDiscretizer(n_bins=5, encode='ordinal'), ['prod_shift2']),\n",
    "\n",
    "        (StandardScaler(), model_num_cols),\n",
    "        \n",
    "        # (StandardScaler(),temp_cols),\n",
    "\n",
    "        # (make_pipeline(StandardScaler(),PCA(n_components=3, random_state=seed)), temp_cols),\n",
    "        \n",
    "        # (KBinsDiscretizer(n_bins=2), [c for c in temp_day_cols if \"Avg\" in c]),\n",
    "\n",
    "        # (StandardScaler(), [c for c in temp_day_cols if \"Avg\" in c]),\n",
    "        # (make_pipeline(StandardScaler(),PCA(n_components=2, random_state=seed)), [c for c in temp_day_cols if \"Avg\" in c]),\n",
    "\n",
    "        # (StandardScaler(),[c for c in meteo_cols if \"avg_daytime\" in c]),\n",
    "\n",
    "        # (make_pipeline(StandardScaler(), PCA(n_components=2, random_state=seed)), [\n",
    "        #  c for c in meteo_cols if \"avg_daytime\" in c]),\n",
    "        \n",
    "        \n",
    "        # (StandardScaler(),precipuo),\n",
    "        # (StandardScaler(),tempuo),\n",
    "        # (StandardScaler(),winduo),\n",
    "        # (StandardScaler(),gustuo),\n",
    "        # (StandardScaler(),[c for c in snowpuo if \"Month1\" in c or \"Month2\" in c or \"Month3\" in c])\n",
    "\n",
    "        (StandardScaler(),[c for c in precipuo if \"2Std\" in c]),\n",
    "        # (StandardScaler(), [c for c in precipuo if \"1Std\" in c]),\n",
    "        # (StandardScaler(),[c for c in winduo if \"2Std\" in c]),\n",
    "        # (StandardScaler(),[c for c in tempuo if \"2Std\" in c]),\n",
    "        # (StandardScaler(),[c for c in gustuo if \"2Std\" in c]),\n",
    "        # (StandardScaler(),[c for c in snowpuo if \"Month1\" in c or \"Month2\" in c or \"Month3\" in c]),\n",
    "\n",
    "        # (make_pipeline(StandardScaler(),PCA(n_components=2, random_state=seed)),\n",
    "            #  [c for c in percip_cols if 'Sum' in c]\n",
    "        # ),\n",
    "\n",
    "        (StandardScaler(),[c for c in snow_cols if 'Sum' in c and (\"1\" in c or \"2\" in c)]),\n",
    "        \n",
    "        # (StandardScaler(), [c for c in gust_cols if 'Max' in c ]),\n",
    "\n",
    "        # (make_pipeline(StandardScaler(),PCA(n_components=2, random_state=seed)), [c for c in gust_cols if 'Max' in c ]),\n",
    "\n",
    "        # (StandardScaler(),[c for c in humid_cols if \"Avg\" in c]),\n",
    "        # (make_pipeline(StandardScaler(),PCA(n_components=2, random_state=seed)), [c for c in humid_cols if \"Avg\" in c]),\n",
    "        # (make_pipeline(StandardScaler(),PCA(n_components=3, random_state=seed)), uvindex_cols),\n",
    "        # (make_pipeline(StandardScaler(),PCA(n_components=3, random_state=seed)), dewpoint_cols),\n",
    "        # (make_pipeline(StandardScaler(),PCA(n_components=2, random_state=seed)), wind_cols),\n",
    "\n",
    "\n",
    "        remainder='drop'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor, VotingRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "prep = get_preprocesing()\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    random_state=seed,\n",
    "    n_estimators=200,\n",
    "    min_samples_leaf=4,\n",
    "    # max_depth=8,\n",
    "    n_jobs=-1,\n",
    "    # max_features=0.22,\n",
    "    max_features=\"sqrt\",\n",
    "    max_samples=0.8\n",
    "    # random_state=seed,\n",
    "    # n_estimators=400,\n",
    "    # n_jobs=-1,\n",
    "    # max_features=1.0,\n",
    "    # max_samples=1.0\n",
    ")\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    # tree_method=\"exact\",\n",
    "    learning_rate=0.009261187281287938,\n",
    "    random_state = seed,\n",
    "    n_estimators = 840,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree = 0.22,\n",
    "    colsample_bynode = 0.70,\n",
    "    # colsample_bylevel = 0.20,\n",
    "    max_depth=6,\n",
    "    # reg_alpha=100,\n",
    "    reg_lambda=22,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "est_xgb = lambda i: xgb.XGBRegressor(\n",
    "    # tree_method=\"exact\",\n",
    "    learning_rate=0.009261187281287938,\n",
    "    random_state=seed+i,\n",
    "    n_estimators=840,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.22,\n",
    "    colsample_bynode=0.70,\n",
    "    # colsample_bylevel = 0.20,\n",
    "    max_depth=6,\n",
    "    # reg_alpha=100,\n",
    "    reg_lambda=22,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "est_rf = lambda i: RandomForestRegressor(\n",
    "    random_state=seed+i,\n",
    "    n_estimators=180,\n",
    "    min_samples_leaf=4,\n",
    "    # max_depth=8,\n",
    "    n_jobs=-1,\n",
    "    # max_features=0.22,\n",
    "    max_features=\"sqrt\",\n",
    "    max_samples=0.8\n",
    "    # random_state=seed,\n",
    "    # n_estimators=400,\n",
    "    # n_jobs=-1,\n",
    "    # max_features=1.0,\n",
    "    # max_samples=1.0\n",
    ")\n",
    "\n",
    "model = VotingRegressor(\n",
    "    estimators=[(f\"est_{i}\", est_xgb(i)) for i in range(5)],\n",
    ")\n",
    "\n",
    "# model = StackingRegressor(\n",
    "#     estimators=[(\"est1\",est_xgb(1)), (\"est2\",est_xgb(0)),(\"est3\",est_rf(0))],\n",
    "#     final_estimator=make_pipeline(StandardScaler(), LinearRegression()),\n",
    "# )\n",
    "\n",
    "m = make_pipeline(\n",
    "    prep,\n",
    "    model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  [-4969.85670136 -4802.25367454 -4909.09924514]\n",
      "test:  [-4837.82060266 -6812.07541638 -5381.24129592]\n",
      "Train Mean RMSE: -4893.736540345112\n",
      "Test Mean RMSE: -5677.045771655104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sample_weight = X['campaña'].apply(lambda x: 0.5 if x == 20 else 1).values\n",
    "# sample_weight = X['campaña'].apply(lambda x: x/22)\n",
    "# sample_weight = X['campaña'].apply(lambda x: 1 if x==20 else 0.5)\n",
    "# sample_weight = X['campaña'].apply(lambda x: 0.2 if x <= 19 else 1)\n",
    "\n",
    "\n",
    "res = cross_validate(\n",
    "    estimator=m,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    scoring=tr.rmse_scorer,\n",
    "    return_train_score=True,\n",
    "    return_estimator=True,\n",
    "    fit_params={\n",
    "        # \"randomforestregressor__sample_weight\": sample_weight\n",
    "        # \"xgbregressor__sample_weight\": sample_weight\n",
    "        # \"stackingregressor__sample_weight\": sample_weight\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"train: \", res['train_score'])\n",
    "print(\"test: \", res['test_score'])\n",
    "print(f\"Train Mean RMSE: {np.mean(res.get('train_score'))}\")\n",
    "print(f\"Test Mean RMSE: {np.mean(res.get('test_score'))}\")\n",
    "\n",
    "\n",
    "# base\n",
    "# train:  [-4950.42915184 - 4769.7101927 - 4818.80517865]\n",
    "# test:  [-4867.31956362 - 6767.87298344 - 5430.5269726]\n",
    "# Train Mean RMSE: -4846.314841064502\n",
    "# Test Mean RMSE: -5688.573173223823\n",
    "\n",
    "# train:  [-4999.3034144 - 4828.81703503 - 4918.20715021]\n",
    "# test:  [-4844.22108053 - 6787.95522349 - 5346.60658675]\n",
    "# Train Mean RMSE: -4915.442533217256\n",
    "# Test Mean RMSE: -5659.594296923198\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "{'xgbregressor__n_estimators': 840}\n",
      "-5659.341004910268\n"
     ]
    }
   ],
   "source": [
    "from numpy import linspace\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \n",
    "    # 'randomforestregressor__max_depth': [4, 5, 6, 8],\n",
    "    # 'randomforestregressor__ccp_alpha': [0,0.01,0.02],\n",
    "\n",
    "\n",
    "    # 'randomforestregressor__max_features': [0.22],\n",
    "    # 'randomforestregressor__max_samples': [0.8], \n",
    "    # 'randomforestregressor__min_samples_leaf':[4],\n",
    "    # 'randomforestregressor__n_estimators': [180,200,300,400,500],\n",
    "\n",
    "    # 'xgbregressor__max_features': [0.22],\n",
    "    # 'xgbregressor__max_samples': [0.8],\n",
    "    # 'xgbregressor__min_samples_leaf': [4],\n",
    "    'xgbregressor__n_estimators': range(820,860,10),\n",
    "    # 'xgbregressor__learning_rate': np.logspace(-2.07, -2, 5), #np.logspace(-2.07, -1.9, 10)\n",
    "}\n",
    "\n",
    "gsm = GridSearchCV(\n",
    "    m,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    scoring=tr.rmse_scorer\n",
    ")\n",
    "\n",
    "gsm.fit(\n",
    "    X, \n",
    "    y, \n",
    "    # randomforestregressor__sample_weight=sample_weight\n",
    "    )\n",
    "\n",
    "print(gsm.best_params_)\n",
    "print(gsm.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgbregressor__n_estimators': 840}\n",
      "-5659.341004910268\n"
     ]
    }
   ],
   "source": [
    "print(gsm.best_params_)\n",
    "print(gsm.best_score_)\n",
    "\n",
    "# {'xgbregressor__learning_rate': 0.009261187281287938}\n",
    "# -5659.594296923198\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  [-4984.0148598  -4812.46729515 -4903.08592288]\n",
      "test:  [-4847.9543898  -6785.57169828 -5344.49692665]\n",
      "Train Mean RMSE: -4899.85602594488\n",
      "Test Mean RMSE: -5659.341004910268\n"
     ]
    }
   ],
   "source": [
    "# m = gsm.best_estimator_\n",
    "# y_pred = m.fit(X_train, y_train).predict(X_test)\n",
    "# score_model(X_test, y_test, y_pred, sup_norm=False)\n",
    "\n",
    "res = cross_validate(\n",
    "    estimator=gsm.best_estimator_,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    scoring=tr.rmse_scorer,\n",
    "    return_train_score=True,\n",
    "    return_estimator=True,\n",
    "    # fit_params={\n",
    "    #     \"randomforestregressor__sample_weight\": sample_weight\n",
    "    # }\n",
    ")\n",
    "\n",
    "print(\"train: \", res['train_score'])\n",
    "print(\"test: \", res['test_score'])\n",
    "print(f\"Train Mean RMSE: {np.mean(res.get('train_score'))}\")\n",
    "print(f\"Test Mean RMSE: {np.mean(res.get('test_score'))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
